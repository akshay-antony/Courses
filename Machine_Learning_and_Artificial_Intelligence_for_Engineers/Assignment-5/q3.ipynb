{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vezeaTvhASid"
   },
   "source": [
    "# Question 3 Flower Classification using CNN \n",
    "- Please **do not** change the default variable names in this problem, as we will use them in different parts.\n",
    "- The default variables are initially set to \"None\".\n",
    "- You only need to modify code in the \"TODO\" part. We added some \"assertions\" to check your code. **Do not** modify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obpyErWGZMyo"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import *\n",
    "import random \n",
    "from tqdm import tqdm \n",
    "import warnings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cA6R2cVIMom"
   },
   "source": [
    "You can upload your image folder on Google drive and access image folder from it. **Skip it if you run on local machine.** To mount google drive to your current colab page, use the following command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niU6h2jiET_8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8slWttWnF930"
   },
   "outputs": [],
   "source": [
    "# check pytorch cuda and use cuda if possible\n",
    "device = torch.cuda.is_available()\n",
    "print('*' * 50)\n",
    "if torch.cuda.is_available():  \n",
    "  print('CUDA is found! Tranining on %s.......'%torch.cuda.get_device_name(0))\n",
    "else:\n",
    "  warnings.warn('CUDA not found! Training may be slow......')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I21FUvr9BCTq"
   },
   "source": [
    "\n",
    "## P1. Data augmentation and plotting\n",
    "### TODO\n",
    "- Design your image augmentation method for transform_image\n",
    "- Load train and test data, and split them into train_loader and test_loader \n",
    "- Visualize your augmented image \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0FPwP6bbJHn"
   },
   "outputs": [],
   "source": [
    "# TODO: define your image augmentation method\n",
    "# Make sure to crop the image in (3,224,224) using transforms.RandomResizedCrop(224)  \n",
    "transform_image = None\n",
    "\n",
    "\n",
    "# TODO: Load data using ImageFolder. Specify your image folder path \n",
    "path = None\n",
    "dataset = datasets.ImageFolder(path,transform=transform_image)\n",
    "\n",
    "n = len(dataset)\n",
    "n_test = int(0.1 * n) \n",
    "\n",
    "# Split data into features(pixels) and labels(numbers from 0 to 4)\n",
    "train_dataset, test_dataset = random_split(dataset, (n-n_test,n_test))\n",
    "train_loader, test_loader = DataLoader(train_dataset, batch_size=16, shuffle=True), DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RXfRWmaJTF_"
   },
   "outputs": [],
   "source": [
    "# Sample output\n",
    "label_map = [['daisy'],['dandelion'],['rose'],['sunflower'],['tulip']]\n",
    "random_image = random.randint(0,len(train_dataset))\n",
    "image = train_dataset.__getitem__(random_image)\n",
    "assert np.array_equal(image[0].detach().numpy().shape, [3,224,224])\n",
    "plt.imshow(image[0].permute(1,2,0))\n",
    "plt.title(f\"Training example {label_map[image[1]]}\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6mtpsr-CR4U"
   },
   "source": [
    "## P2. Build you own CNN model \n",
    "### TODO\n",
    "- Design your own model class in **CNNModel(nn.Module)** and write forward pass in **forward(self, x)**\n",
    "- Create loss function in **error**, optimizer in **optimizer**\n",
    "- Define hyparparameters: **learning_rate**, **num_epochs**\n",
    "- Plot your **loss vs num_epochs** and **accuracy vs num_epochs** \n",
    "- Plot your first convolution layer kernels using **plot_filters_multi_channel()**\n",
    "\n",
    "###  Hints\n",
    "- Start with low number of epochs for debugging. (eg. num_epochs=1)\n",
    "- You may want to use small learning rate for training. (eg. 1e-5)\n",
    "- Be careful with the input dimension of fully connected layer. \n",
    "- The dimension calculation of the output tensor from the input tensor is \\\\\n",
    "$D_{out}=\\frac{D_{in}-K+2P}{S}+1$ \\\\\n",
    "$D_{out}$ : Dimension of output tensor \\\\\n",
    "$D_{in}$ : Dimension of input tensor \\\\\n",
    "$K$ : width/height of the kernel \\\\\n",
    "$S$ : stride \\\\\n",
    "$P$ : padding\n",
    "\n",
    "## Convolutional and Pooling Layers\n",
    "\n",
    "A convolutional layer using pyTorch:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "torch.nn.Conv2d(num_in_channels, num_out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "```\n",
    "For example:\n",
    "```\n",
    "torch.nn.Conv2d(3, 32, 3)\n",
    "```\n",
    "It applies a 2D convolution over an input signal composed of several input planes. If we have input size with $(N, C_{in}, H, W)$ and output size with $(N, C_{out}, H_{out}, W_{out})$, the 2D convolution can described as $$out(N_i, C_{out_j}) = bias(C_{out_j}) + \\sum_{k=0}^{C_{in}-1}weight(C_{out_j},k)\\star input(N_i,k)$$\n",
    "\n",
    "**num_in_channels:** is the number of channels of the input tensor. If the previous layer is the input layer, num_in_channels is the number of channels of the image (3 channels for RGB images), otherwise num_in_channels is equal to the number of feature maps of the previous layer.\n",
    "\n",
    "**num_out_channels:** is the number of filters (feature extractor) that this layer will apply over the image or feature maps generated by the previous layer.\n",
    "\n",
    "**kernel_size:** is the size of the convolving kernel\n",
    "So for instance, if we have an RGB image and we are going to apply 32 filters of 3x3:\n",
    "\n",
    "**stide:** is the stride of the convolution. Default: 1\n",
    "\n",
    "**padding:** is the padding added to all four sides of the input. Default: 0\n",
    "\n",
    "**dilation:** is the spacing between kernel elements. Default: 1\n",
    "\n",
    "**group:** is the number of blocked connections from input channels to output channels. Default: 1\n",
    "\n",
    "**bias:** If True, adds a learnable bias to the output. Default: True\n",
    "\n",
    "## A Simple Convolutional Neural Network\n",
    "\n",
    "In our convnet we'll use the next structure shown in the comment:\n",
    "\n",
    "*input -> convolution -> pooling -> fully connected -> output* \\\\\n",
    "\n",
    "**Convolution #1**\n",
    "\n",
    "16 kernels of 5x5; *Width/Height:* (224 - 5 + 2x0) / 1 + 1 = 220; *Output dimensions:* (16, 220, 220)\n",
    "\n",
    "**Max Pooling #1**\n",
    "\n",
    "filter size = 2, stride = 2; *Width/Height:* (220 - 2) / 2 + 1 = 110; *Output dimensions:* (16, 110, 110)\n",
    "\n",
    "So at the end of the last convolutional layer we get a tensor of dimension (16, 110, 110). And since now we are going to feed it to fully connected classifier, we need to convert it into a 1-D vector, and for that we use the reshape method:\n",
    "\n",
    "```\n",
    "x = x.view(x.size(0), -1)\n",
    "```\n",
    "The way of calculating size of the output size from previous convolution layer can be formulized as below: $$H_{output} = \\frac{H_{in}+2\\times padding-kernel\\_Size}{stride}+1$$\n",
    "\n",
    "For more details, you can refer to this link: \\\\\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrvosNegfTaU"
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CNNModel, self).__init__()\n",
    "    # TODO: Create CNNModel using 2D convolution. You should vary the number of convolution layers and fully connected layers \n",
    "    # Example:  \n",
    "    # self.cnn1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "    # self.relu1 = nn.ReLU() \n",
    "    # self.maxpool1 = nn.MaxPool2d(kernel_size=2) \n",
    "    self.cnn1 = None     \n",
    "    \n",
    "    # TODO: Create Fully connected layers. You should calculate the dimension of the input tensor from the previous layer \n",
    "    # Example: \n",
    "    # self.fc1 = nn.Linear(16 *110 * 110, 5)\n",
    "    # Fully connected 1\n",
    "    self.fc1 = None\n",
    "\n",
    "  def forward(self,x):\n",
    "    # TODO: Perform forward pass in blow section \n",
    "    # Example:\n",
    "    # out = self.cnn1(x)\n",
    "    # out = self.relu1(out)      \n",
    "    # out = self.maxpool1(out) \n",
    "    # out = out.view(out.size(0), -1)\n",
    "    # out = self.fc1(out)\n",
    "    \n",
    "    out = None\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa6w_m3WAz3P"
   },
   "source": [
    "## Starting Up Our Model\n",
    "\n",
    "We'll send the model to our GPU if you have one so we need to create a CUDA device and instantiate our model. Then we will define the loss function and \n",
    "hyperparameters that we need to train the model: \\\\\n",
    "\n",
    "###TODO\n",
    "- Define Cross Entropy Loss\n",
    "- Create Adam Optimizer\n",
    "- Define hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfJseAOtgA_N"
   },
   "outputs": [],
   "source": [
    "# Create CNN\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CNNModel()\n",
    "model.to(device)\n",
    "\n",
    "# TODO: define Cross Entropy Loss \n",
    "error = None\n",
    "\n",
    "# TODO: create Adam Optimizer and define your hyperparameters \n",
    "learning_rate = None\n",
    "optimizer = None\n",
    "num_epochs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiIreB-YAdV4"
   },
   "source": [
    "### Training the Model\n",
    "### TODO \n",
    "- Make predictions from your model\n",
    "- Calculate Cross Entropy Loss from predictions and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYHpNCnogtNQ"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TODO: Forward propagation\n",
    "        outputs = None\n",
    "        \n",
    "        # TODO: Calculate softmax and ross entropy loss\n",
    "        loss = None\n",
    "        \n",
    "        # Backprop agate your Loss \n",
    "        loss.backward()\n",
    "        \n",
    "        # Update CNN model  \n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if count % 50 == 0:\n",
    "            model.eval()\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward propagation\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.argmax(outputs,1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += len(labels)\n",
    "                \n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / float(total)\n",
    "            \n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        if count % 500 == 0:\n",
    "            # Print Loss\n",
    "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uz-jrjkDg4Uq"
   },
   "outputs": [],
   "source": [
    "# visualization loss\n",
    "plt.plot(iteration_list,loss_list)\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"CNN: Loss vs Number of iteration\")\n",
    "plt.show()\n",
    "\n",
    "# visualization accuracy \n",
    "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"CNN: Accuracy vs Number of iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MSn7UzbAn5d"
   },
   "source": [
    "### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUxl95m5IQnY"
   },
   "outputs": [],
   "source": [
    "# Evaluate your model\n",
    "random_image = random.randint(0,len(train_dataset))\n",
    "image = train_dataset.__getitem__(random_image)\n",
    "model.eval()\n",
    "images, labels = next(iter(train_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "predictions = torch.argmax(model(images),1)\n",
    "num_cols=1\n",
    "num_rows = len(labels)\n",
    "fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "for idx in range(num_rows):\n",
    "  ax1 = fig.add_subplot(num_rows,num_cols,idx+1)\n",
    "  img = images.cpu().detach()[idx].numpy()\n",
    "  img = (img - np.mean(img)) / np.std(img)\n",
    "  img = np.minimum(1, np.maximum(0, (img + 0.5)))\n",
    "  ax1.imshow(img.transpose((1,2,0)))\n",
    "  \n",
    "  ax1.set_title(f\"Label {label_map[labels[idx]]}, Prediction {label_map[predictions[idx]]}\")\n",
    "  ax1.axis('off')\n",
    "plt.savefig('Prediction.png', dpi=100)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWLN1TdaEXxr"
   },
   "source": [
    "### Visualizing your first layer filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubS7w99ZrLkI"
   },
   "outputs": [],
   "source": [
    "# plot your first layer kernels \n",
    "\n",
    "def plot_filters_multi_channel(t):\n",
    "    #make sure the input channel is 3 \n",
    "    assert(t.shape[1]==3)\n",
    "\n",
    "    #get the number of kernals\n",
    "    num_kernels = t.shape[0]    \n",
    "    \n",
    "    #define number of columns for subplots\n",
    "    num_cols = 12\n",
    "\n",
    "    #rows = num of kernels\n",
    "    num_rows = num_kernels\n",
    "    \n",
    "    #set the figure size\n",
    "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
    "    \n",
    "    #looping through all the kernels\n",
    "    for i in range(t.shape[0]):\n",
    "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
    "        \n",
    "        #for each kernel, we convert the tensor to numpy \n",
    "        npimg = np.array(t[i].cpu().detach().numpy(), np.float32)\n",
    "        \n",
    "        #standardize the numpy image\n",
    "        npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
    "        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
    "        npimg = npimg.transpose((1, 2, 0))\n",
    "        ax1.imshow(npimg)\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title(str(i))\n",
    "        ax1.set_xticklabels([])\n",
    "        ax1.set_yticklabels([])\n",
    "        \n",
    "    plt.savefig('Filter.png', dpi=100)    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_filters_multi_channel(list(model.parameters())[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW5_P3_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
